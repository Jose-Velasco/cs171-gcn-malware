# %%
import torch
from torch.utils.data import Subset
from datasets import APICallGraphDataset
from pathlib import Path
from models import GCN
from torch_geometric.loader import DataLoader
from utils import generateFamilyLabels, generateLabelToIndex
from torchmetrics import MetricCollection
from torchmetrics.classification import (
    MulticlassAccuracy, MulticlassAUROC, MulticlassConfusionMatrix,
    MulticlassF1Score, MulticlassPrecision, MulticlassRecall, MulticlassROC,
    MulticlassPrecisionRecallCurve
    )
import matplotlib.pyplot as plt
import numpy as np

# %%
rawDataPath = Path("./AllFiles_CleanLogAPI/raw")
familyLabels = generateFamilyLabels(rawDataPath)
labelsToIndices = generateLabelToIndex(familyLabels)

# %%
dataset = APICallGraphDataset(
    root="./AllFiles_CleanLogAPI",
    apiCallEmbeddingsFilePath="./APICallword2vec_VecSize100_skipGram.wordvectors",
    weighted=True,
    allClassLabels=labelsToIndices
)

# %%
# DATA_SET_NUM_FEATURES = dataset.num_features
# DATA_SET_CLASSES = dataset.num_classes
DATA_SET_NUM_FEATURES = 100
DATA_SET_CLASSES = 7
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(12345)


# %%
# yTorch Geometric provides some useful utilities for working with graph datasets, e.g., we can shuffle the dataset and use the first
TRAIN_INDICES_FNAME = "./dataSplits/undirectedWeightedTrainIndicesSplits.npy"
VALIDATION_INDICES_FNAME = "./dataSplits/undirectedWeightedValidationIndicesSplits.npy"
TEST_INDICES_FNAME = "./dataSplits/undirectedWeightedTestIndicesSplits.npy"

# train_indices, val_indices, test_indices = balanceDataSplits(labelsToIndices, dataset, 0.8, 0.1)
train_indices = np.load(TRAIN_INDICES_FNAME)
val_indices = np.load(VALIDATION_INDICES_FNAME)
test_indices = np.load(TEST_INDICES_FNAME)

# dataset = dataset.shuffle()
# train_dataset = dataset[:392]
# test_dataset = dataset[392:]

# %%
# Use Subset to create subsets of the dataset
train_dataset = Subset(dataset, train_indices)
val_dataset = Subset(dataset, val_indices)
test_dataset = Subset(dataset, test_indices)

# %%
LEN_TRAIN_DATASET = len(train_dataset)
LEN_VAL_DATASET = len(val_dataset) 
LEN_TEST_DATASET = len(test_dataset)

print(f'Number of training graphs: {LEN_TRAIN_DATASET}')
print(f'Number of validation graphs: {LEN_VAL_DATASET}')
print(f'Number of test graphs: {LEN_TEST_DATASET}')
print(f'Total graphs across all 3 datasets: {LEN_TEST_DATASET + LEN_VAL_DATASET + LEN_TRAIN_DATASET}')

# %%
# Mini-batching of graphs
# The length of this dimension is then equal to the number of examples grouped in a
# mini-batch and is typically referred to as the batch_size
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

for step, data in enumerate(train_loader):
    print(f'Step {step + 1}:')
    print('=======')
    print(f'Number of graphs in the current batch: {data.num_graphs}')
    print(data)
    print()

# Here, we opt for a batch_size of 64, leading to 5 (randomly shuffled) mini-batches, containing all  5⋅64 + 9 =150  graphs.
# Furthermore, each Batch object is equipped with a batch vector, which maps each node to its respective graph in the batch:
# batch=[0,…,0,1,…,1,2,…]

# %%
# Training a GNN for graph classification usually follows a simple recipe:
# 1. Embed each node by performing multiple rounds of message passing
# 2. Aggregate node embeddings into a unified graph embedding (readout layer)
# 3. Train a final classifier on the graph embedding

def train(trainLoader: DataLoader, device) -> float:
    model.train()
    total_train_loss = 0.0
    correct = 0
    for data in trainLoader:  # Iterate in batches over the training dataset.
        data = data.to(device)
        out = model(data.x, data.edge_index, data.edge_attr, data.batch)  # Perform a single forward pass.
        loss = criterion(out, data.y)  # Compute the loss.
        loss.backward()  # Derive gradients.
        optimizer.step()  # Update parameters based on gradients.
        optimizer.zero_grad()  # Clear gradients.
        pred = out.argmax(dim=1)  # Use the class with highest probability.
        correct += int((pred == data.y).sum())  # Check against ground-truth labels.
        total_train_loss += loss
    average_train_loss = total_train_loss / len(trainLoader.dataset)
    accuracy_train = correct / len(trainLoader.dataset)  # Derive ratio of correct predictions.
    return average_train_loss, accuracy_train

def validationPhase(val_loader: DataLoader, metrics: MetricCollection, device):
    model.eval()
    total_val_loss = 0.0
    correct_val = 0
    with torch.no_grad():
        for batch in val_loader:
            # Similar to the training loop, perform forward pass, compute loss, and accuracy
            batch = batch.to(device)

            outputs = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            loss = criterion(outputs, batch.y)

            metrics(outputs, batch.y) # pass prediction and label to metrics object to calculate metrics
            pred = outputs.argmax(dim=1)  # Use the class with highest probability.
            correct_val += (pred == batch.y).sum()
            total_val_loss += loss


    average_val_loss = total_val_loss / len(val_loader.dataset)
    accuracy_val = correct_val / len(val_loader.dataset)
    return average_val_loss, accuracy_val

def testPhase(test_loader: DataLoader, device):
    model.eval()
    total_test_loss = 0.0
    correct_test = 0
    with torch.no_grad():
        for batch in test_loader:
            # Similar to the validation loop, perform forward pass, compute loss, and accuracy
            batch = batch.to(device)

            outputs = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            loss = criterion(outputs, batch.y)

            pred = outputs.argmax(dim=1)  # Use the class with the highest probability.
            correct_test += (pred == batch.y).sum()
            total_test_loss += loss

    average_test_loss = total_test_loss / len(test_loader.dataset)
    accuracy_test = correct_test / len(test_loader.dataset)
    return average_test_loss, accuracy_test

# %%
# def test(loader, device):
#     model.eval()
#     correct = 0
#     for data in loader:  # Iterate in batches over the training/test dataset.
#         data.to(device)    
#         out = model(data.x, data.edge_index, data.edge_attr, data.batch)
#         pred = out.argmax(dim=1)  # Use the class with highest probability.
#         correct += int((pred == data.y).sum())  # Check against ground-truth labels.
#     return correct / len(loader.dataset)  # Derive ratio of correct predictions.

# def evlMetrics(metrics: MetricCollection, loader: DataLoader, device):
#     model.eval()
#     for data in loader:
#         data.to(device)

#         out = model(data.x, data.edge_index, data.edge_attr, data.batch)
#         metrics(out, data.y)


# %%
metricCollection = MetricCollection([
    MulticlassAccuracy(num_classes=DATA_SET_CLASSES, average="macro"),
    MulticlassROC(num_classes=DATA_SET_CLASSES, thresholds=None),
    MulticlassPrecisionRecallCurve(num_classes=DATA_SET_CLASSES, thresholds=None),
    MulticlassAUROC(num_classes=DATA_SET_CLASSES, average=None, thresholds=None),
    MulticlassConfusionMatrix(num_classes=DATA_SET_CLASSES),
    MulticlassF1Score(num_classes=DATA_SET_CLASSES),
    MulticlassPrecision(num_classes=DATA_SET_CLASSES),
    MulticlassRecall(num_classes=DATA_SET_CLASSES)
])
metricCollection.to(DEVICE)
# %%
model = GCN(DATA_SET_NUM_FEATURES, DATA_SET_CLASSES, 128).to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()
print(model)

# %%
NUM_EPOCHS = 30
averageValLosses: list[float] = []
averageTrainLoesses: list[float] = []
for epoch in range(1, NUM_EPOCHS + 1):
    average_train_loss, accuracy_train = train(train_loader, DEVICE)
    average_val_loss, accuracy_val = validationPhase(val_loader, metricCollection, DEVICE)
    averageTrainLoesses.append(average_train_loss)
    averageValLosses.append(average_val_loss)
    # train_acc = test(train_loader, DEVICE)
    # evlMetrics(metricCollection, train_loader, DEVICE)
    # test_acc = test(test_loader, device)
    # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')
    # print(f"epoch {epoch}")
    trainAverageAccStr = f"average over each label Acc: {accuracy_train:.4f}"
    trainLossStr = f"Train Loss: {average_train_loss:.4f}"
    valLossStr = f'Validation Loss: {average_val_loss:.4f}'
    valAccuracyStr = f'Validation Accuracy: {accuracy_val:.4f}'

    print(f"Epoch: {epoch:03d}/{NUM_EPOCHS}: {trainAverageAccStr}, {trainLossStr}, {valLossStr}, {valAccuracyStr}\n")

# %%[markdown]
#  ## Results

# %%
test_loss, test_accuracy = testPhase(test_loader, DEVICE)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%')

# %%[markdown]

# ## Visualize the results

# %%
# loss graph
fig, ax = plt.subplots()
ax.plot(torch.tensor(averageTrainLoesses), label='Training loss')
ax.plot(torch.tensor(averageValLosses), label='Validation loss')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss percent')
ax.set_title('Train Loss V.S. Validation Loss')
ax.set_ylim(-0.1, 1.1)
ax.legend()  # Show legend if labels are provided
# Add a horizontal line for every y-tick
for y_tick in ax.get_yticks():
    ax.axhline(y_tick, color='gray', linestyle='-', linewidth=0.5)

# Add a dashed horizontal line at y=0.0
ax.axhline(0.0, color='grey', linestyle='--', linewidth=2)
ax.axhline(1.0, color='grey', linestyle='--', linewidth=2)
ax.text(0.2, 0.002, 'Optimal\nvalue', color='black', fontsize=10, ha='center', va='center')


# Show the plot
plt.show()
# %%

for metric in metricCollection.values():
    # print(metric.plot.__annotations__)
    if "score" in metric.plot.__annotations__:
        fig, ax = metric.plot(score=True)
    else:
        fig, ax = metric.plot()
    plt.show()


# %%
