from pathlib import Path
from typing import Iterable
from torch_geometric.data import Dataset, Data
from torch_geometric.loader import DataLoader
from torch.nn import Module, CrossEntropyLoss
from torch.optim import Optimizer
from torch import no_grad, tensor
from torchmetrics import Metric
import matplotlib.pyplot as plt
from torchmetrics.wrappers import MetricTracker
import json
import os


def generateFamilyLabels(rawDataPath: Path) -> list[str]:
    familyLabels = [dirEntryLabel.stem.lower().strip().strip("\n") for dirEntryLabel in rawDataPath.iterdir() if dirEntryLabel.is_dir()]
    return familyLabels

def generateLabelToIndex(familyLabels: list[str]) -> dict[str, int]:
    label_to_index = {label: index for index, label in enumerate(familyLabels)}
    return label_to_index

def balanceDataSplits(unique_labels: dict[str, int], dataset: Dataset, trainPercent: float, valPercent: float):
    train_indices, val_indices, test_indices = [], [], []
    # Iterate over each class and split indices for each subset
    for labelKey in unique_labels:
        idx = 0
        label_indices = []
        labelIdx = unique_labels[labelKey]
        for data in dataset:
            if (data.y == labelIdx):
                label_indices.append(idx)
            idx += 1

        num_data = len(label_indices)
        num_train = int(trainPercent * num_data)
        num_val = int(valPercent * num_data)
        train_indices.extend(label_indices[:num_train])
        val_indices.extend(label_indices[num_train:num_train + num_val])
        test_indices.extend(label_indices[num_train + num_val:])
        remainder = num_data - (num_train + num_val)
        if remainder > 0:
            print(f"Warning: Class {labelKey} has {remainder} samples left out.")

    return train_indices, val_indices, test_indices

class EarlyStopper:
    def __init__(self, patience: int = 1, min_delta: int = 0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = float('inf')

    def early_stop(self, validation_loss: float):
        if validation_loss < self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss > (self.min_validation_loss + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False
    
def train(model: Module, criterion: CrossEntropyLoss, optimizer: Optimizer, trainLoader: DataLoader, device) -> float:
    model.train()
    total_train_loss = 0.0
    correct = 0
    for data in trainLoader:  # Iterate in batches over the training dataset.
        data = data.to(device)
        out = model(data.x, data.edge_index, data.edge_attr, data.batch)  # Perform a single forward pass.
        loss = criterion(out, data.y)  # Compute the loss.
        loss.backward()  # Derive gradients.
        optimizer.step()  # Update parameters based on gradients.
        optimizer.zero_grad()  # Clear gradients.
        pred = out.argmax(dim=1)  # Use the class with highest probability.
        correct += int((pred == data.y).sum())  # Check against ground-truth labels.
        total_train_loss += loss
    average_train_loss = total_train_loss / len(trainLoader.dataset)
    accuracy_train = correct / len(trainLoader.dataset)  # Derive ratio of correct predictions.
    return average_train_loss, accuracy_train

def validationPhase(model: Module, criterion: CrossEntropyLoss, val_loader: DataLoader, metricsTracker: MetricTracker, device):
    model.eval()
    total_val_loss = 0.0
    correct_val = 0
    with no_grad():
        for batch in val_loader:
            # Similar to the training loop, perform forward pass, compute loss, and accuracy
            batch = batch.to(device)

            outputs = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            loss = criterion(outputs, batch.y)

            metricsTracker.update(outputs, batch.y) # pass prediction and label to metrics object to calculate metrics
            pred = outputs.argmax(dim=1)  # Use the class with highest probability.
            correct_val += (pred == batch.y).sum()
            total_val_loss += loss


    average_val_loss = total_val_loss / len(val_loader.dataset)
    accuracy_val = correct_val / len(val_loader.dataset)
    return average_val_loss, accuracy_val

def testPhase(model: Module, criterion: CrossEntropyLoss, test_loader: DataLoader, device):
    model.eval()
    total_test_loss = 0.0
    correct_test = 0
    with no_grad():
        for batch in test_loader:
            # Similar to the validation loop, perform forward pass, compute loss, and accuracy
            batch = batch.to(device)

            outputs = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            loss = criterion(outputs, batch.y)

            pred = outputs.argmax(dim=1)  # Use the class with the highest probability.
            correct_test += (pred == batch.y).sum()
            total_test_loss += loss

    average_test_loss = total_test_loss / len(test_loader.dataset)
    accuracy_test = correct_test / len(test_loader.dataset)
    return average_test_loss, accuracy_test

def generateAllPlots(metrics: Iterable[Metric], savePathDirectory: str | None = None):
    for metric in metrics:
        if "score" in metric.plot.__annotations__:
            fig, ax = metric.plot(score=True)
        else:
            fig, ax = metric.plot()
            lines = ax.lines
            # very hard coded
            if len(lines) == 1 or len(line.get_ydata() == 7):
                for line in lines:
                    x_data = line.get_xdata()
                    y_data = line.get_ydata()
                    
                    # Annotate each point with its value
                    for xi, yi in zip(x_data, y_data):
                        ax.annotate(f'{yi:.2f}', (xi, yi), textcoords="offset points", xytext=(0, 10), ha='center')

        if savePathDirectory:
            fig.savefig(f"{savePathDirectory}/{metric.__class__.__name__}.png")
        plt.show()

def plotTrainVSValidationLoss(trainLoss: list[float], valLoss: list[float], savePathDirectory: str | None =None):
    fig, ax = plt.subplots()
    ax.plot(tensor(trainLoss), label='Training loss')
    ax.plot(tensor(valLoss), label='Validation loss')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss percent')
    ax.set_title('Train Loss V.S. Validation Loss')
    ax.set_ylim(-0.1, 1.1)
    ax.legend()  # Show legend if labels are provided
    # Add a horizontal line for every y-tick
    for y_tick in ax.get_yticks():
        ax.axhline(y_tick, color='gray', linestyle='-', linewidth=0.5)

    # Add a dashed horizontal line at y=0.0
    ax.axhline(0.0, color='grey', linestyle='--', linewidth=2)
    ax.axhline(1.0, color='grey', linestyle='--', linewidth=2)
    ax.text(0.2, 0.002, 'Optimal\nvalue', color='black', fontsize=10, ha='center', va='center')
    if savePathDirectory:
        fig.savefig(f"{savePathDirectory}/{fig.axes[0].get_title()}.png")
    plt.show()

def load_dict_from_file(filename):
    try:
        with open(filename, 'r') as json_file:
            return json.load(json_file)
    except FileNotFoundError:
        return None

def has_changed(original, current):
    return original != current

def save_dict_if_changed(current, filename):
    # Load existing dictionary from the file
    existing_dict = load_dict_from_file(filename)

    # Check if the dictionary has changed
    if has_changed(existing_dict, current):
        with open(filename, 'w') as json_file:
            json.dump(current, json_file)
        print(f"Dictionary has not changed. Dictionary saved to {filename}")
    else:
        print("Dictionary has not changed. No need to save.")